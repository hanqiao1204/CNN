# CNN

This is one of my projects at the Deep Learning course where I built my a CNN model on the CIFAR-10 dataset with 0.8282 accuracy on the validation dataset. I started transfer learning and I referred the VGGNet as my starting model architecture (Very Deep Convolutional Networks for Large-Scale Image Recognition, https://arxiv.org/abs/1409.1556). In that paper, it used 4 blocks of convolutional layers ( each includes 2 convolutional layers + 1 max pooling with ReLu activation) and fully connected layers on the 224 * 224 images with 1000 classes. In the CIFAR-10, the images are 32 * 32, indicating that we may need less convolutional layers to reach a good accuracy in our case. Also, I want to select 2 fully connected layers and an output layer with softmax in order to fully connected the complicated features after convolutional layers . In terms of the number of filters in each layer, I chose to keep them same with VGG, which are also common options in CNN architecture. Therefore, I selected 3 blocks of convolutional layers (2 convolutional layers + 1 maxpooling with ReLu activation) and 3 fully connected layers (both with ReLu activation) adding drop-out and Batch norm in the optimization.

In the experiments, I found the accuracy is not very sensitive to the kernel size in the max pooling layer and it’s more sensitive to learning rate and kernel size in convolutional layer. So I kept kernel_size = 2 and stride = 2 in max pooling layer, the same in VGGNet. Also, I chose the padding = ’same’ in order to keep dimensions unchanged and learn the edges better. As I reduced convolutional layers in this case, I wanted increase kernel size in convolutional layers (not kernel size = 3 in VGGNet). First, I chose kernel size = 3 and learning rate = 0.0001, which reached accuracy under 0.5. Then I increased learning rate to and model’s accuracy has reached over 0.5. Since I used less number of convolutional layers than VGG, I need to increase kernel size in order to learn this complex relationship. At the same time, I added small regularization (drop out) in order to prevent overfitting. Finally, I selected learning rate = 0.003 and kernel size = 7 in the all convolutional layers. 

